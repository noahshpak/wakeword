{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_asr.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oE3sSwiDdYB"
      },
      "source": [
        "# **Command Word Detection Using Speech-to-Text**\n",
        "Automatic speech recognition (ASR) involves transcribing audio into text. ASR is a sequence-to-sequence problem where the audio signal is a sequence of feature vectors and the text is a sequence of characters, words, or tokens.\n",
        "\n",
        "ASR shows promise as a flexible command word detection system. Many wake word detectors are optimized for a single word or phrase. In this notebook, we will implement a wake word detector based on the Transformer model detailed in [*Attention Is All You Need*](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
        "\n",
        "Using a transformer model architecture allows us to identify specific commands in speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O1i0HlCq9pfy",
        "outputId": "f8577ef1-3173-4a3f-efcd-dc88a26a1701"
      },
      "source": [
        "!pip install audiomentations"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting audiomentations\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/e1/3078fe444be2a100d804ee1296115367c27fa1dfa6298bf4155f77345822/audiomentations-0.16.0-py3-none-any.whl\n",
            "Requirement already satisfied: librosa<=0.8.0,>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (0.8.0)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from audiomentations) (1.19.5)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.10.3.post1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.22.2.post1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (0.51.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (2.1.9)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (1.0.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<=0.8.0,>=0.6.1->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.14.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (20.9)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.4.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa<=0.8.0,>=0.6.1->audiomentations) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations) (56.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa<=0.8.0,>=0.6.1->audiomentations) (0.34.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa<=0.8.0,>=0.6.1->audiomentations) (2.4.7)\n",
            "Installing collected packages: audiomentations\n",
            "Successfully installed audiomentations-0.16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7z7dDLjKagS"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from glob import glob\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display\n",
        "from audiomentations import Compose, AddBackgroundNoise, AddGaussianNoise, AddShortNoises, Gain, PitchShift"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im1J-18kKpae"
      },
      "source": [
        "# Set a random seed for reproducibility:\n",
        "random_seed = 10\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n4dU-InXxLs"
      },
      "source": [
        "## **1. Importing the Dataset**\n",
        "In this section we'll import the [*LJSpeech dataset*](https://www.tensorflow.org/datasets/catalog/ljspeech). This dataset contains `.wav` audio files with sentences read from various books by a single speaker. The data was collected by LibriVox and is under public domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2wOepOU5hxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d0a8e4-2d90-4f03-e033-c1edb7b1df42"
      },
      "source": [
        "# Get the data:\n",
        "keras.utils.get_file(\n",
        "    os.path.join(os.getcwd(),\"data.tar.gz\"),\n",
        "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
        "    extract=True,\n",
        "    archive_format=\"tar\",\n",
        "    cache_dir=\".\"\n",
        ")\n",
        "\n",
        "saveto = \"./datasets/LJSpeech-1.1\"\n",
        "wavs = glob(\"{}/**/*.wav\".format(saveto), recursive=True)\n",
        "\n",
        "id_to_text = {}\n",
        "with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
        "  for line in f:\n",
        "    id = line.strip().split(\"|\")[0]\n",
        "    text = line.strip().split(\"|\")[2]\n",
        "    id_to_text[id] = text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "2748579840/2748572632 [==============================] - 61s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hER9FgQbLDwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebce0399-da08-4326-cae0-32ab7e05f523"
      },
      "source": [
        "# Set up the default path for the speech commands data:\n",
        "data_dir = pathlib.Path(\"data/mini_speech_commands\")\n",
        "\n",
        "# Check if the data exists locally:\n",
        "if not os.path.exists(data_dir):\n",
        "  # Get the files:\n",
        "  tf.keras.utils.get_file(\n",
        "      \"mini_speech_commands.zip\",\n",
        "      origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "      extract=True,\n",
        "      cache_dir=\".\",\n",
        "      cache_subdir=\"data\"\n",
        "  )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\n",
            "182083584/182082353 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw3-VdKxYvKk"
      },
      "source": [
        "def get_data(wavs, id_to_text, max_length=50):\n",
        "  data = []\n",
        "  for w in wavs:\n",
        "    id = w.split(\"/\")[-1].split(\".\")[0]\n",
        "    if len(id_to_text[id]) < max_length:\n",
        "      data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
        "\n",
        "  return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp4bHiM-7n1l"
      },
      "source": [
        "class VectorizeChar:\n",
        "  def __init__(self, max_length=50):\n",
        "    self.vocab = (\n",
        "        [\"-\", \"#\", \"<\", \">\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\" \", \".\", \",\", \"?\"]\n",
        "        )\n",
        "    self.max_length = max_length\n",
        "    self.char_to_idx = {}\n",
        "    for j, ch in enumerate(self.vocab):\n",
        "        self.char_to_idx[ch] = j\n",
        "  \n",
        "  def __call__(self, text):\n",
        "    text = text.lower()\n",
        "    text = text[: self.max_length - 2]\n",
        "    text = \"<\" + text + \">\"\n",
        "    pad_length = self.max_length - len(text)\n",
        "    return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_length\n",
        "\n",
        "  def get_vocabulary(self):\n",
        "    return self.vocab"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqR412ITNw0l"
      },
      "source": [
        "We'll now check out the dataset to ensure the data loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovV9tQ-g8QdH",
        "outputId": "ba2e26ea-0588-414f-90ea-25feeb5f92f2"
      },
      "source": [
        "max_target_length = 200\n",
        "data = get_data(wavs, id_to_text, max_target_length)\n",
        "vectorizer = VectorizeChar(max_target_length)\n",
        "print(\"Vocab size: {}\".format(len(vectorizer.get_vocabulary())))\n",
        "print(\"Dataset size: {}\".format(len(data)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 34\n",
            "Dataset size: 13100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h73B_udXZnHl"
      },
      "source": [
        "## **2. Data Preparation & Analysis**\n",
        "Our next step is to prepare speech data for analysis and model training. Audio files are read as binary. We need to convert each file to a numerical tensor.\n",
        "\n",
        "We use the [`tf.audio.decode_wav`](https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav) function to load the audio files as Tensors with a specified sampling rate.\n",
        "\n",
        "The `.wav` files contain time series data with a specific number of samples per second. Each sample represents the audio signal amplitude at a given time. Files in the `LJSpeech` dataset are sampled at 16 bits and range from -32768 to 32767. The sampling rate is 16kHz.\n",
        "\n",
        "Note that `tf.audio.decode_wav` will normalize values to a range of [-1.0, 1.0]. We apply various augmentations (e.g., gain, pitch shift) to the recorded `LJSpeech` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYiqvD8B8bz-"
      },
      "source": [
        "def create_text_dataset(data):\n",
        "  texts = [_[\"text\"] for _ in data]\n",
        "  text_data = [vectorizer(t) for t in texts]\n",
        "  text_data = tf.data.Dataset.from_tensor_slices(text_data)\n",
        "  return text_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fshQxBJA-dbF"
      },
      "source": [
        "def path_to_audio(path, sample_rate=16000, augmentation=False):\n",
        "  audio = tf.io.read_file(path)\n",
        "  audio, _ = tf.audio.decode_wav(audio, 1)\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
        "  x = tf.math.pow(tf.abs(stfts), 0.5)\n",
        "  means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
        "  stdvs = tf.math.reduce_std(x, 1, keepdims=True)\n",
        "  x = (x - means) / stdvs\n",
        "  audio_length = tf.shape(x)[0]\n",
        "  pad_length = 2754\n",
        "  paddings = tf.constant([[0, pad_length], [0, 0]])\n",
        "  x = tf.pad(x, paddings, \"CONSTANT\")[:pad_length, :]\n",
        "  if augmentation and x.shape[0] is not None:\n",
        "    transformations = Compose([\n",
        "        AddGaussianNoise(),\n",
        "        Gain(),\n",
        "        PitchShift()\n",
        "    ])\n",
        "    x = transformations(x, sample_rate=sample_rate)\n",
        "  else:\n",
        "    pass\n",
        "  return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW22lZqs8vFK"
      },
      "source": [
        "def create_audio_dataset(data, augmentation=False):\n",
        "  files = [_[\"audio\"] for _ in data]\n",
        "  audio_data = tf.data.Dataset.from_tensor_slices(files)\n",
        "  audio_data = audio_data.map(\n",
        "      lambda a: path_to_audio(a, augmentation=augmentation), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "  return audio_data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p88IcRSx9K6U"
      },
      "source": [
        "def create_tf_dataset(data, batch_size=4, augmentation=False):\n",
        "  audio_data = create_audio_dataset(data, augmentation)\n",
        "  text_data = create_text_dataset(data)\n",
        "  data = tf.data.Dataset.zip((audio_data, text_data))\n",
        "  data = data.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "  data = data.batch(batch_size)\n",
        "  data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return data"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U71fwtxM9WnY"
      },
      "source": [
        "split = int(len(data) * 0.99)\n",
        "train_data = data[:split]\n",
        "test_data = data[split:]\n",
        "raw_train_dataset = create_tf_dataset(train_data, batch_size=64)\n",
        "aug_train_dataset = create_tf_dataset(train_data, batch_size=64, augmentation=True)\n",
        "train_dataset = raw_train_dataset.concatenate(aug_train_dataset)\n",
        "test_dataset = create_tf_dataset(test_data, batch_size=4)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlJ8Rt2TlLmc"
      },
      "source": [
        "## **3. Model Definition**\n",
        "Transformer models for ASR have been detailed in [*Attention Is All You Need*](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). These models map one sequence (e.g., audio samples) to another (e.g., characters in text).\n",
        "\n",
        "Transformers consist of an Encoder layer and a Decoder layer. The Encoder maps an input sequence to a set of features. The Decoder maps features to an output sequence of interest. In our case, the Encoder will map audio sample data to features which will be mapped to characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jN-oM7RoZ1e"
      },
      "source": [
        "### **3.1 Transformer Input Layer**\n",
        "In this section we'll define input layers for the Transformer model. The character parsing will use a sum of positional and token embeddings. The audio parsing will apply convolutional layers to downsample the input signals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGpf5CsWmWx2"
      },
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "  def __init__(self, vocab_size=1000, max_length=100, num_hid=64):\n",
        "    super(TokenEmbedding, self).__init__()\n",
        "    # Define the token embedding layer:\n",
        "    self.token_embedding = tf.keras.layers.Embedding(vocab_size, num_hid)\n",
        "    # Define the position embedding layer:\n",
        "    self.position_embedding = tf.keras.layers.Embedding(max_length, num_hid)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Get the input sequence length:\n",
        "    max_length = tf.shape(x)[-1]\n",
        "    # Generate a token embedding:\n",
        "    x = self.token_embedding(x)\n",
        "    # Get the positions in the sequence:\n",
        "    p = tf.range(start=0, limit=max_length, delta=1)\n",
        "    # Generate a position embedding:\n",
        "    p = self.position_embedding(p)\n",
        "    return x + p"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2hN4i-2ncKR"
      },
      "source": [
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "  def __init__(self, num_hid, max_length=100, kernel_size=11, strides=2):\n",
        "    super(SpeechFeatureEmbedding, self).__init__()\n",
        "    # Define the convolutional layers:\n",
        "    self.conv1 = tf.keras.layers.Conv1D(num_hid, \n",
        "                                        kernel_size=kernel_size, \n",
        "                                        strides=strides, \n",
        "                                        padding=\"same\", \n",
        "                                        activation=\"relu\")\n",
        "    self.conv2 = tf.keras.layers.Conv1D(num_hid, \n",
        "                                        kernel_size=kernel_size, \n",
        "                                        strides=strides, \n",
        "                                        padding=\"same\", \n",
        "                                        activation=\"relu\")\n",
        "    self.conv3 = tf.keras.layers.Conv1D(num_hid, \n",
        "                                        kernel_size=kernel_size, \n",
        "                                        strides=strides, \n",
        "                                        padding=\"same\", \n",
        "                                        activation=\"relu\")\n",
        "    # Define the positional embedding:\n",
        "    self.position_embedding = layers.Embedding(max_length, num_hid)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    return x"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGL0npVSo7tY"
      },
      "source": [
        "### **3.2 Transformer Encoder Layer**\n",
        "We'll now define the Encoder layer. The Encoder maps input sequences to low-level features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu10vh2ioRz7"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    # Define the attention layer:\n",
        "    self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    # Define the feed-forward layer:\n",
        "    self.ffn = keras.Sequential(\n",
        "        [\n",
        "         layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim),\n",
        "        ]\n",
        "    )\n",
        "    # Define the normalization layers:\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    # Define the dropout layers:\n",
        "    self.dropout1 = layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Get the output of the multi-head attention layer:\n",
        "    attn_output = self.attn(inputs, inputs)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "    # Get the output of the feed-forward layer:\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)\n",
        "    return out2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU_6HkmPsIhr"
      },
      "source": [
        "### **3.3 Transformer Decoder Layer**\n",
        "The Decoder maps encoded features to an output sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiBfducGrFQa"
      },
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    # Define the self attention layer:\n",
        "    self.self_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    # Define the encoder attention layer:\n",
        "    self.enc_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    # Define the feed-forward layer:\n",
        "    self.ffn = keras.Sequential(\n",
        "        [\n",
        "         layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "         layers.Dense(embed_dim),\n",
        "        ]\n",
        "    )\n",
        "    # Define the normalization layers:\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    # Define the dropout layers:\n",
        "    self.self_dropout = layers.Dropout(0.5)\n",
        "    self.enc_dropout = layers.Dropout(dropout_rate)\n",
        "    self.ffn_dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "  def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "    '''\n",
        "    Masks the upper half of the dot product matrix when computing\n",
        "    self-attention.\n",
        "\n",
        "    This prevents the flow of information from future tokens to\n",
        "    the current token.\n",
        "    '''\n",
        "    i = tf.range(n_dest)[:,None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, axis=-1)\n",
        "        ,tf.constant([1,1], dtype=tf.int32)]\n",
        "        ,0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "  def call(self, enc_out, target):\n",
        "    # Shape the input sequence:\n",
        "    input_shape = tf.shape(target)\n",
        "    # Get the batch size:\n",
        "    batch_size = input_shape[0]\n",
        "    # Get the length of each sequence in the batch:\n",
        "    seq_len = input_shape[1]\n",
        "    # Generate the causal attention mask:\n",
        "    causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "    # Apply the self-attention layer with the mask:\n",
        "    target_attn = self.self_attn(target, target, attention_mask=causal_mask)\n",
        "    # Normalize the self-attention output:\n",
        "    target_norm = self.layernorm1(target + self.self_dropout(target_attn))\n",
        "    # Apply the encoder attention layer:\n",
        "    enc_out = self.enc_attn(target_norm, enc_out)\n",
        "    # Normalize the encoder attention output:\n",
        "    enc_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "    # Apply the feed-forward layer:\n",
        "    ffn_out = self.ffn(enc_norm)\n",
        "    # Normalize the feed-forward output:\n",
        "    ffn_norm = self.layernorm3(enc_norm + self.ffn_dropout(ffn_out))\n",
        "    return ffn_norm"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpIwHJV4wWJe"
      },
      "source": [
        "### **3.4 Full Transformer Model**\n",
        "We now have the components to assemble our Transformer model.\n",
        "\n",
        "The model takes audio spectrograms as inputs and predicts a sequence of characters. During training, we give the decoder a left-shifted target character sequence. During inference, the decoder uses its own past predictions to predict the next token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTI6hZKhw1Cp"
      },
      "source": [
        "class Transformer(keras.Model):\n",
        "  def __init__(self,\n",
        "               num_hid=64,\n",
        "               num_heads=2,\n",
        "               num_feed_forward=128,\n",
        "               src_max_length=100,\n",
        "               tgt_max_length=100,\n",
        "               num_enc_layers=4,\n",
        "               num_dec_layers=1,\n",
        "               num_classes=10,\n",
        "               ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "    self.num_enc_layers = num_enc_layers\n",
        "    self.num_dec_layers = num_dec_layers\n",
        "    self.tgt_max_length = tgt_max_length\n",
        "    self.num_classes = num_classes\n",
        "    self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, max_length=src_max_length)\n",
        "    self.dec_input = TokenEmbedding(vocab_size=num_classes, max_length=tgt_max_length, num_hid=num_hid)\n",
        "    self.encoder = keras.Sequential(\n",
        "        [self.enc_input]\n",
        "        + [TransformerEncoder(embed_dim=num_hid, \n",
        "                              num_heads=num_heads, \n",
        "                              feed_forward_dim=num_feed_forward) for _ in range(num_enc_layers)]\n",
        "    )\n",
        "    for j in range(num_dec_layers):\n",
        "      setattr(self,\n",
        "              f\"dec_layer_{j}\",\n",
        "              TransformerDecoder(embed_dim=num_hid, \n",
        "                                 num_heads=num_heads, \n",
        "                                 feed_forward_dim=num_feed_forward),\n",
        "      )\n",
        "    self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "  def decode(self, enc_out, tgt):\n",
        "    # Transform the input:\n",
        "    y = self.dec_input(tgt)\n",
        "    # Loop through the decoder layers:\n",
        "    for j in range(self.num_dec_layers):\n",
        "      # Get the decoder:\n",
        "      dec = getattr(self,f\"dec_layer_{j}\")\n",
        "      # Decode the data:\n",
        "      y = dec(enc_out, y)\n",
        "    return y\n",
        "\n",
        "  def call(self, inp):\n",
        "    # Get the source sequence in the input data:\n",
        "    src = inp[0]\n",
        "    # Get the target sequence in the input data:\n",
        "    tgt = inp[1]\n",
        "    # Encode the source data:\n",
        "    x = self.encoder(src)\n",
        "    # Decode the taret data:\n",
        "    y = self.decode(x, tgt)\n",
        "    # Apply the classifier:\n",
        "    out = self.classifier(y)\n",
        "    return out\n",
        "  \n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [self.loss_metric]\n",
        "  \n",
        "  def train_step(self, batch):\n",
        "    src = batch[\"source\"]\n",
        "    tgt = batch[\"target\"]\n",
        "    dec_input = tgt[:,:-1]\n",
        "    dec_target = tgt[:,1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = self([src, dec_input])\n",
        "      one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "      mask = tf.math.logical_not(tf.math.equal(dec_target,0))\n",
        "      loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    self.loss_metric.update_state(loss)\n",
        "    return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "  def test_step(self, batch):\n",
        "    src = batch[\"source\"]\n",
        "    tgt = batch[\"target\"]\n",
        "    dec_input = tgt[:,:-1]\n",
        "    dec_target = tgt[:,1:]\n",
        "    preds = self([src,dec_input])\n",
        "    one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "    mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "    loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "    self.loss_metric.update_state(loss)\n",
        "    return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "  def generate(self, source, target_start_token_idx):\n",
        "    bs = tf.shape(source)[0]\n",
        "    enc = self.encoder(source)\n",
        "    dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "    dec_logits = []\n",
        "    for j in range(self.tgt_max_length - 1):\n",
        "      dec_out = self.decode(enc, dec_input)\n",
        "      logits = self.classifier(dec_out)\n",
        "      logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "      last_logit = tf.expand_dims(logits[:,-1], axis=-1)\n",
        "      dec_logits.append(last_logit)\n",
        "      dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "    return dec_input"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrPOB-elbny6"
      },
      "source": [
        "## **4. Training the Model**\n",
        "Our final step is to train the Transformer model. We define a [`Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) class extension to display model results every few epochs. We also create a custom [`Learning Rate Schedule`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule) to adjust the learning rate and mitigate overfitting.\n",
        "\n",
        "The best model results are saved to a checkpoint directory. Results are checked after each training epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjJ7Ltid_R1e"
      },
      "source": [
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "  def __init__(\n",
        "      self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
        "  ):\n",
        "    self.batch = batch\n",
        "    self.target_start_token_idx = target_start_token_idx\n",
        "    self.target_end_token_idx = target_end_token_idx\n",
        "    self.idx_to_token = idx_to_token\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if (1 + epoch) % 5 !=0:\n",
        "      return\n",
        "    else:\n",
        "      source = self.batch[\"source\"]\n",
        "      target = self.batch[\"target\"]\n",
        "      batch_size = tf.shape(source)[0]\n",
        "      preds = self.model.generate(source, self.target_start_token_idx)\n",
        "      preds = preds.numpy()\n",
        "      for j in range(batch_size):\n",
        "        target_text = \"\".join([self.idx_to_token[t] for t in target[j,:]])\n",
        "        pred = \"\"\n",
        "        for idx in preds[j,:]:\n",
        "          pred += self.idx_to_token[idx]\n",
        "          if idx == self.target_end_token_idx:\n",
        "            break\n",
        "        print(\"Target:      {}\".format(target_text.replace(\"-\",\"\")))\n",
        "        print(\"Prediction:  {}\\n\".format(pred))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDojVttFA_7c"
      },
      "source": [
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      init_lr=1e-5,\n",
        "      lr_after_warmup=1e-3,\n",
        "      final_lr=1e-5,\n",
        "      warmup_epochs=15,\n",
        "      decay_epochs=85,\n",
        "      steps_per_epoch=203,\n",
        "  ):\n",
        "    super(CustomSchedule).__init__()\n",
        "    self.init_lr = init_lr\n",
        "    self.lr_after_warmup = lr_after_warmup\n",
        "    self.final_lr = final_lr\n",
        "    self.warmup_epochs = warmup_epochs\n",
        "    self.decay_epochs = decay_epochs\n",
        "    self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "  def calculate_lr(self, epoch):\n",
        "    warmup_lr = self.init_lr + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "    decay_lr = tf.math.maximum(\n",
        "        self.final_lr,\n",
        "        self.lr_after_warmup - (epoch - self.warmup_epochs) * (self.lr_after_warmup - self.final_lr) / self.decay_epochs\n",
        "    )\n",
        "    return tf.math.minimum(warmup_lr, decay_lr)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    epoch = step // self.steps_per_epoch\n",
        "    return self.calculate_lr(epoch)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "zM7AlT5hCI0-",
        "outputId": "ff6e6855-81e2-4e35-9c92-19902d6a0392"
      },
      "source": [
        "batch = next(iter(test_dataset))\n",
        "\n",
        "# Set up the callback to display model predictions:\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_callback = DisplayOutputs(batch,\n",
        "                                  idx_to_char,\n",
        "                                  target_start_token_idx=2,\n",
        "                                  target_end_token_idx=3)\n",
        "\n",
        "# Set up the callback to save model checkpoints:\n",
        "checkpoint_path = \"./model/training/checkpoints\"\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                         monitor=\"val_loss\",\n",
        "                                                         save_freq=\"epoch\",\n",
        "                                                         mode=\"min\",\n",
        "                                                         save_weights_only=True,\n",
        "                                                         save_best_only=True,\n",
        "                                                         verbose=0)\n",
        "\n",
        "# Define the transformer model:\n",
        "model = Transformer(\n",
        "    num_hid = 200,\n",
        "    num_heads=2,\n",
        "    num_feed_forward=400,\n",
        "    tgt_max_length=max_target_length,\n",
        "    num_enc_layers=4,\n",
        "    num_dec_layers=1,\n",
        "    num_classes=34\n",
        ")\n",
        "\n",
        "# Load the model weights:\n",
        "if os.path.exists(checkpoint_path) and len(os.listdir(checkpoint_path)) > 0:\n",
        "  model.load_weights(checkpoint_path)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "# Define the loss function:\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "\n",
        "# Define the learning rate schedule:\n",
        "lr = CustomSchedule(\n",
        "    init_lr = 5e-5,\n",
        "    lr_after_warmup=1e-3,\n",
        "    final_lr=1e-5,\n",
        "    warmup_epochs=15,\n",
        "    decay_epochs=135,\n",
        "    steps_per_epoch=len(train_dataset)\n",
        ")\n",
        "\n",
        "# Define the model optimizer:\n",
        "optimizer = keras.optimizers.Adam(lr)\n",
        "\n",
        "# Compile the model:\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "# Train the model:\n",
        "hist = model.fit(train_dataset, validation_data=test_dataset, callbacks=[display_callback, checkpoint_callback], epochs=150)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "145/406 [=========>....................] - ETA: 3:53 - loss: 1.5300"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1f9ab952fb54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Train the model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisplay_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}